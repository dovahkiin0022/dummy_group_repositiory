{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "groupProject_hyperparameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQxR1PAVZrvP5dwFrWDNCM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dovahkiin0022/dummy_group_repositiory/blob/trial_seda/groupProject_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter Tuning**\n",
        "\n",
        "Hyperparameter tuning is an optimization problem, like model training.\n",
        "\n",
        "List of hyperparameters for a regression model:\n",
        "\n",
        "1. "
      ],
      "metadata": {
        "id": "KHi2kwIGeY4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters to be tuned\n",
        "params = {'n_ch': 16, 'width': 5, 'n_conv': 4}"
      ],
      "metadata": {
        "id": "N08OhtRsks2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset\n",
        "\n",
        "*--> the code for loading the data will be added here after preprocessing.*"
      ],
      "metadata": {
        "id": "L7vEfQCIjiax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n",
        "\n",
        "*--> After deciding the model and the features (input-output), the code for model will be added here.*"
      ],
      "metadata": {
        "id": "PHzRS_lKiNmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ...\n",
        "model = ...\n",
        "\n",
        "def train_model():\n",
        "\n",
        "  loss = \n",
        " return loss"
      ],
      "metadata": {
        "id": "dF7JwzZdlKiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Jz2AZm6lWPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search\n",
        "Define a search space as a grid ('param_grid') of hyperparameter values and evaluate every position in the grid."
      ],
      "metadata": {
        "id": "rL0qD-_tiUJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boYIIit6dzpt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def grid_search():\n",
        "\n",
        "  #Define cross validation method if you prefer. Otherwise, it is a number (currently 10).\n",
        "  gs = GridSearchCV(model, params, scoring='neg_mean_absolute_percentage_error', \n",
        "                    n_jobs=-1, cv=10)\n",
        "\n",
        "  # Perform Grid Search using train input and output\n",
        "  result = gs.fit(X_train, y_train)\n",
        "\n",
        "  print('Best Score: %0.4f' % result.best_score_)\n",
        "  print('Best Hyperparameters: %s' % result.best_params_)\n",
        "\n",
        "  # Return the best hyperparameters\n",
        "  return result.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Search\n",
        "Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain."
      ],
      "metadata": {
        "id": "VSw5nnw2kiZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def random_search():\n",
        "\n",
        "  #Define cross validation method if you prefer. Otherwise, it is a number (currently 10).\n",
        "  rs = RandomizedSearchCV(model, params, scoring='neg_mean_absolute_percentage_error', \n",
        "                          n_jobs=-1, cv=10)\n",
        "\n",
        "  # Perform Random Search using train input and output\n",
        "  result = rs.fit(X_train, y_train)\n",
        "\n",
        "  print('Best Score: %0.4f' % result.best_score_)\n",
        "  print('Best Hyperparameters: %s' % result.best_params_)\n",
        "\n",
        "  # Return the best hyperparameters\n",
        "  return result.best_params_"
      ],
      "metadata": {
        "id": "ORHAhjC9k4qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Genetic Algorithm"
      ],
      "metadata": {
        "id": "kH1RJ7JIcwe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters required to be defined for each problem according to paper (https://arxiv.org/pdf/2106.06158.pdf):\n",
        "1. num_generations: The number of generations/iterations.\n",
        "2. sol_per_pop: The number of solutions/chromosomes/indi-\n",
        "viduals in the population (i.e. population size).\n",
        "3. num_parents_mating: The number of solutions to be se- lected from the population as parents for mating and producing the offspring.\n",
        "4. num_genes: The number of genes in each solution.\n",
        "5. fitness_func: The fitness function."
      ],
      "metadata": {
        "id": "1l6Yfr8rdRkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pygad"
      ],
      "metadata": {
        "id": "v36vXE4vdGRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitness function evaluates the performance of the mutations and the operations in genetic algorithm. To do so, a metric (value) must be defined. This value can be the loss value during training of the model.\n",
        "\n",
        "Since the aim is to minimize the loss during training and to increase the fitness value in the genetic algorithm, in order not to get mathematical error in case of very low loss values, the fitness value can be defined as 1/loss. "
      ],
      "metadata": {
        "id": "_v15z3B8ddaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygad\n",
        "\n",
        "def fitness_func(solution, solution_idx):\n",
        "  loss = train_model(**params)\n",
        "  fitness=1/loss.item()\n",
        "  return fitness\n",
        "\n",
        "def genetic_lagorithm():\n",
        "\n",
        "  # Define genetic algorithm parameters\n",
        "  ga_instance = pygad.GA(num_generations=10, \n",
        "                        num_parents_mating=3,\n",
        "                        sol_per_pop=5,\n",
        "                        num_genes=3,\n",
        "                        fitness_func=fitness_func)\n",
        "\n",
        "  ga_instance.run()\n",
        "\n",
        "  # Get the best parameters, fitness value and the index of the generation for the best parameters\n",
        "  solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "\n",
        "  print('Best Score: %0.4f' % solution_fitness)\n",
        "  print('Best Hyperparameters: %s' % solution)\n",
        "\n",
        "  # Return the best hyperparameters\n",
        "  return solution"
      ],
      "metadata": {
        "id": "8c4ZFCpTfJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the performance of genetic lagorithm during iterations\n",
        "fig = ga_instance.plot_result()"
      ],
      "metadata": {
        "id": "IeJ7ibdggbQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian Optimization"
      ],
      "metadata": {
        "id": "3uw8-SNbhexw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "np4ZFJbfggWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def loss():\n",
        "\n",
        "    loss = train_model(**params)\n",
        "    return -loss\n",
        "\n",
        "def bayesian_opt():\n",
        "  # Define bounds for the model parameters to be optimized\n",
        "  pbounds = {'E':(1e6,70e6), 'nu':(0, 0.5), 'C1':(5e6, 15e6), 'C2':(0.01e6, 5e6), 'C3':(0.01e6, 5e6)}\n",
        "\n",
        "  # Define bayesian optimizer\n",
        "  optimizer = BayesianOptimization(\n",
        "      f=loss,\n",
        "      pbounds=pbounds,\n",
        "      random_state=42\n",
        "      )\n",
        "\n",
        "  # Define bayesian optimizer properties\n",
        "  optimizer.maximize(init_points=5, n_iter=15, acq='ei', xi=1e-2)\n",
        "\n",
        "  print('Best Score: %0.4f' % result.best_score_) # find attribute for best score\n",
        "  print('Best Hyperparameters: %s' % optimizer.max)\n",
        "\n",
        "  #return the best hyperparameters\n",
        "  return optimizer.max"
      ],
      "metadata": {
        "id": "IYE-mo2tji1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization\n",
        "\n",
        "Compare the performance of the model different hyperparameter tuning "
      ],
      "metadata": {
        "id": "lKPhXnWcnOpA"
      }
    }
  ]
}